{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_EasyOCR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOdGND8UEE3TGmsuClb9dk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GruAna/VU/blob/master/train_EasyOCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EasyOCR"
      ],
      "metadata": {
        "id": "8iuzqAo45UM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "QPru2sDLX4xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2 as cv"
      ],
      "metadata": {
        "id": "bkyZyokQ5u_o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4OFu2Y3jrBl",
        "outputId": "c8880f63-95d8-4c3c-82fe-c782d47ddfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Colab_Notebooks/VU/utils.py /content\n",
        "!mv /content/utils.py /content/utilities.py"
      ],
      "metadata": {
        "id": "bE9mwtlBpQRe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import *"
      ],
      "metadata": {
        "id": "vAGrLMv4_JfY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "0W9WiVwRx-xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loading (training data)"
      ],
      "metadata": {
        "id": "lhPOAy5ZYF39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CTW1500 dataset**"
      ],
      "metadata": {
        "id": "t4ceHCnUZFEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get images. Update location of images **manually**."
      ],
      "metadata": {
        "id": "97DN9W0tPrGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to image directory, get full path to all files\n",
        "imgs_dir = '/content/drive/MyDrive/Colab_Notebooks/VU/FewImages/images/'\n",
        "(_, _, filenames) = next(os.walk(imgs_dir))\n",
        "filenames.sort()\n",
        "list_img_paths = [os.path.join(imgs_dir, file) for file in filenames]\n",
        "n_imgs = len(list_img_paths)"
      ],
      "metadata": {
        "id": "k8mtKHdykOfq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load images\n",
        "train_images = [(cv.imread(file)) for file in list_img_paths]"
      ],
      "metadata": {
        "id": "2nrizBXRY-Pb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get paths to files with labels. Update location **manually**."
      ],
      "metadata": {
        "id": "rLIx5BFtziDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to label directory, get full path to all files\n",
        "labels_dir = '/content/drive/MyDrive/Colab_Notebooks/VU/FewImages/labelsxml/'\n",
        "(_, _, xml_files) = next(os.walk(labels_dir))\n",
        "xml_files.sort()\n",
        "list_xml_paths = [os.path.join(labels_dir, file) for file in xml_files]"
      ],
      "metadata": {
        "id": "KbLbqMD_Apaq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get ground truths from all xml files"
      ],
      "metadata": {
        "id": "rttWVgWP1Dg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth = []\n",
        "\n",
        "for i, file in enumerate(list_xml_paths):\n",
        "     ground_truth.append(read_gt_ctw_train(file))\n",
        "\n",
        "# ground_truth is in the is a list of tuples, where first is the gt word \n",
        "# and second is an array of top left and bottom right coordinates\n",
        "# format: ('text', [[tl,tl],[br,br]])"
      ],
      "metadata": {
        "id": "yjB3g4kN1BeZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crop images with multiple words to one image = one word, save and create corresponding label .txt file (one for all images, called `gt.txt`)"
      ],
      "metadata": {
        "id": "zUOeZv_qx8Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_text_crop(train_images, filenames, ground_truth)"
      ],
      "metadata": {
        "id": "DNfzqFayx9Wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b087b5ad-1b54-4872-9aad-f52e16aee06d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [00:01, 10.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert gt.txt to labels.csv"
      ],
      "metadata": {
        "id": "nHq-NSQaoZh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('./results/gt.txt', 'r') as infile, open('./results/labels.csv', 'w') as outfile:\n",
        "     stripped = (line.strip() for line in infile)\n",
        "     lines = (line.split(\"\\t\") for line in stripped if line)\n",
        "     writer = csv.writer(outfile)\n",
        "     writer.writerow(['filename', 'words'])\n",
        "     writer.writerows(lines)"
      ],
      "metadata": {
        "id": "Zs21WaZ-D5J8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone EasyOCR trainer"
      ],
      "metadata": {
        "id": "yjqVmqE6teFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/JaidedAI/EasyOCR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGFO76-itd0I",
        "outputId": "d51db7fa-53c0-493f-a7d9-ed59070deb95"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EasyOCR'...\n",
            "remote: Enumerating objects: 2237, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 2237 (delta 0), reused 1 (delta 0), pack-reused 2235\u001b[K\n",
            "Receiving objects: 100% (2237/2237), 148.54 MiB | 15.18 MiB/s, done.\n",
            "Resolving deltas: 100% (1364/1364), done.\n",
            "Checking out files: 100% (255/255), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ./EasyOCR/trainer/all_data/en_val\n",
        "! mkdir ./EasyOCR/trainer/all_data/en_train_filtered"
      ],
      "metadata": {
        "id": "gruFkFl9t3ov"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/results/labels.csv /content/EasyOCR/trainer/all_data/en_val\n",
        "! cp /content/results/labels.csv /content/EasyOCR/trainer/all_data/en_train_filtered\n",
        "! cp /content/results/*.tif /content/EasyOCR/trainer/all_data/en_train_filtered\n",
        "! cp /content/results/*.tif /content/EasyOCR/trainer/all_data/en_val"
      ],
      "metadata": {
        "id": "-JeaMjMwFeaL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "% cd ./EasyOCR/trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdeulug7ubKQ",
        "outputId": "a6f96ace-2fb1-4399-8bce-44fec6c505cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EasyOCR/trainer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "### Training setup"
      ],
      "metadata": {
        "id": "dEpf9HQ61z4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "import yaml\n",
        "from train import train\n",
        "from utils import AttrDict\n",
        "import pandas as pd\n",
        "cudnn.benchmark = True\n",
        "cudnn.deterministic = False"
      ],
      "metadata": {
        "id": "9geAJewDoTea"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config(file_path):\n",
        "    with open(file_path, 'r', encoding=\"utf8\") as stream:\n",
        "        opt = yaml.safe_load(stream)\n",
        "    opt = AttrDict(opt)\n",
        "    if opt.lang_char == 'None':\n",
        "        characters = ''\n",
        "        for data in opt['select_data'].split('-'):\n",
        "            csv_path = os.path.join(opt['train_data'], data, 'labels.csv')\n",
        "            df = pd.read_csv(csv_path, sep='^([^,]+),', engine='python', usecols=['filename', 'words'], keep_default_na=False)\n",
        "            all_char = ''.join(df['words'])\n",
        "            characters += ''.join(set(all_char))\n",
        "        characters = sorted(set(characters))\n",
        "        opt.character= ''.join(characters)\n",
        "    else:\n",
        "        opt.character = opt.number + opt.symbol + opt.lang_char\n",
        "    os.makedirs(f'./saved_models/{opt.experiment_name}', exist_ok=True)\n",
        "    return opt\n",
        "opt = get_config(\"config_files/en_filtered_config.yaml\")\n",
        "train(opt, amp=False)"
      ],
      "metadata": {
        "id": "ToHmyAuIQkkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275f9d8e-1dbd-4112-af2c-acacc12c7b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering the images containing characters which are not in opt.character\n",
            "Filtering the images whose label is longer than opt.batch_max_length\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root: all_data\n",
            "opt.select_data: ['en_train_filtered']\n",
            "opt.batch_ratio: ['1']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data\t dataset: en_train_filtered\n",
            "all_data/en_train_filtered\n",
            "sub-directory:\t/en_train_filtered\t num samples: 178\n",
            "num total samples of en_train_filtered: 178 x 1.0 (total_data_usage_ratio) = 178\n",
            "num samples of en_train_filtered per batch: 32 x 1.0 (batch_ratio) = 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Total_batch_size: 32 = 32\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    all_data/en_val\t dataset: /\n",
            "all_data/en_val/\n",
            "sub-directory:\t/.\t num samples: 178\n",
            "--------------------------------------------------------------------------------\n",
            "No Transformation module specified\n",
            "model input parameters 64 600 20 1 256 256 97 34 None VGG BiLSTM CTC\n",
            "Model:\n",
            "DataParallel(\n",
            "  (module): Model(\n",
            "    (FeatureExtraction): VGG_FeatureExtractor(\n",
            "      (ConvNet): Sequential(\n",
            "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): ReLU(inplace=True)\n",
            "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (7): ReLU(inplace=True)\n",
            "        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (9): ReLU(inplace=True)\n",
            "        (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "        (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (13): ReLU(inplace=True)\n",
            "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (16): ReLU(inplace=True)\n",
            "        (17): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "        (18): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (19): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
            "    (SequenceModeling): Sequential(\n",
            "      (0): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "      (1): BidirectionalLSTM(\n",
            "        (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "        (linear): Linear(in_features=512, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (Prediction): Linear(in_features=256, out_features=97, bias=True)\n",
            "  )\n",
            ")\n",
            "Modules, Parameters\n",
            "module.FeatureExtraction.ConvNet.0.weight 288\n",
            "module.FeatureExtraction.ConvNet.0.bias 32\n",
            "module.FeatureExtraction.ConvNet.3.weight 18432\n",
            "module.FeatureExtraction.ConvNet.3.bias 64\n",
            "module.FeatureExtraction.ConvNet.6.weight 73728\n",
            "module.FeatureExtraction.ConvNet.6.bias 128\n",
            "module.FeatureExtraction.ConvNet.8.weight 147456\n",
            "module.FeatureExtraction.ConvNet.8.bias 128\n",
            "module.FeatureExtraction.ConvNet.11.weight 294912\n",
            "module.FeatureExtraction.ConvNet.12.weight 256\n",
            "module.FeatureExtraction.ConvNet.12.bias 256\n",
            "module.FeatureExtraction.ConvNet.14.weight 589824\n",
            "module.FeatureExtraction.ConvNet.15.weight 256\n",
            "module.FeatureExtraction.ConvNet.15.bias 256\n",
            "module.FeatureExtraction.ConvNet.18.weight 262144\n",
            "module.FeatureExtraction.ConvNet.18.bias 256\n",
            "module.SequenceModeling.0.rnn.weight_ih_l0 262144\n",
            "module.SequenceModeling.0.rnn.weight_hh_l0 262144\n",
            "module.SequenceModeling.0.rnn.bias_ih_l0 1024\n",
            "module.SequenceModeling.0.rnn.bias_hh_l0 1024\n",
            "module.SequenceModeling.0.rnn.weight_ih_l0_reverse 262144\n",
            "module.SequenceModeling.0.rnn.weight_hh_l0_reverse 262144\n",
            "module.SequenceModeling.0.rnn.bias_ih_l0_reverse 1024\n",
            "module.SequenceModeling.0.rnn.bias_hh_l0_reverse 1024\n",
            "module.SequenceModeling.0.linear.weight 131072\n",
            "module.SequenceModeling.0.linear.bias 256\n",
            "module.SequenceModeling.1.rnn.weight_ih_l0 262144\n",
            "module.SequenceModeling.1.rnn.weight_hh_l0 262144\n",
            "module.SequenceModeling.1.rnn.bias_ih_l0 1024\n",
            "module.SequenceModeling.1.rnn.bias_hh_l0 1024\n",
            "module.SequenceModeling.1.rnn.weight_ih_l0_reverse 262144\n",
            "module.SequenceModeling.1.rnn.weight_hh_l0_reverse 262144\n",
            "module.SequenceModeling.1.rnn.bias_ih_l0_reverse 1024\n",
            "module.SequenceModeling.1.rnn.bias_hh_l0_reverse 1024\n",
            "module.SequenceModeling.1.linear.weight 131072\n",
            "module.SequenceModeling.1.linear.bias 256\n",
            "module.Prediction.weight 24832\n",
            "module.Prediction.bias 97\n",
            "Total Trainable Params: 3781345\n",
            "Trainable params num :  3781345\n",
            "Optimizer:\n",
            "Adadelta (\n",
            "Parameter Group 0\n",
            "    eps: 1e-08\n",
            "    lr: 1.0\n",
            "    rho: 0.95\n",
            "    weight_decay: 0\n",
            ")\n",
            "------------ Options -------------\n",
            "number: 0123456789\n",
            "symbol: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €\n",
            "lang_char: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "experiment_name: en_filtered\n",
            "train_data: all_data\n",
            "valid_data: all_data/en_val\n",
            "manualSeed: 1111\n",
            "workers: 6\n",
            "batch_size: 32\n",
            "num_iter: 300000\n",
            "valInterval: 20000\n",
            "saved_model: \n",
            "FT: False\n",
            "optim: False\n",
            "lr: 1.0\n",
            "beta1: 0.9\n",
            "rho: 0.95\n",
            "eps: 1e-08\n",
            "grad_clip: 5\n",
            "select_data: ['en_train_filtered']\n",
            "batch_ratio: ['1']\n",
            "total_data_usage_ratio: 1.0\n",
            "batch_max_length: 34\n",
            "imgH: 64\n",
            "imgW: 600\n",
            "rgb: False\n",
            "contrast_adjust: 0.0\n",
            "sensitive: True\n",
            "PAD: True\n",
            "data_filtering_off: False\n",
            "Transformation: None\n",
            "FeatureExtraction: VGG\n",
            "SequenceModeling: BiLSTM\n",
            "Prediction: CTC\n",
            "num_fiducial: 20\n",
            "input_channel: 1\n",
            "output_channel: 256\n",
            "hidden_size: 256\n",
            "decode: greedy\n",
            "new_prediction: False\n",
            "freeze_FeatureFxtraction: False\n",
            "freeze_SequenceModeling: False\n",
            "character: 0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ €ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "num_class: 97\n",
            "---------------------------------------\n",
            "\n",
            "training time:  10393.300442695618\n",
            "[20000/300000] Train loss: 0.20256, Valid loss: 0.00004, Elapsed_time: 10393.30272\n",
            "Current_accuracy : 100.000, Current_norm_ED  : 1.0000\n",
            "Best_accuracy    : 100.000, Best_norm_ED     : 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Ground Truth              | Prediction                | Confidence Score & T/F\n",
            "--------------------------------------------------------------------------------\n",
            "619 pieces                | 619 pieces                | 0.5024\tTrue\n",
            "8798524231                | 8798524231                | 0.8656\tTrue\n",
            "--------------------------------------------------------------------------------\n",
            "validation time:  3.818570852279663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "8HrAd3NvvpjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}